# Method 3: Separate PI and RI. Use our intuition/belief to arbitrarily weigh the components, and modify the weights with Bayesian inference.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


def _safe_corr(x: np.ndarray, y: np.ndarray) -> float:
    """Pearson correlation with safety checks."""
    if len(x) < 3:
        return np.nan
    sx = np.std(x, ddof=1)
    sy = np.std(y, ddof=1)
    if sx == 0 or sy == 0 or np.isnan(sx) or np.isnan(sy):
        return np.nan
    return float(np.corrcoef(x, y)[0, 1])


def _event_pi_ri(ofi_series: pd.Series) -> dict:
    """
    Compute PI and RI for a single event from an OFI time series (ordered by time).
    Returns dict with PI, RI and component values for debugging.
    """
    x = ofi_series.to_numpy(dtype=float)
    T = len(x)

    # Sign: treat exact zero as 0
    s = np.sign(x)  # -1, 0, +1
    a = np.abs(x)

    # --- PI components ---
    # (1) Level autocorr rho^(1) = corr(x_t, x_{t-1})
    rho1 = _safe_corr(x[1:], x[:-1])
    rho1_01 = (rho1 + 1.0) / 2.0 if not np.isnan(rho1) else np.nan

    # (2) Directional consistency: fraction of consecutive days with same nonzero sign
    nonzero_pair = (s[1:] != 0) & (s[:-1] != 0)
    if nonzero_pair.any():
        C = float(np.mean((s[1:][nonzero_pair] == s[:-1][nonzero_pair])))
    else:
        C = np.nan

    # (3) Drift ratio: |sum x| / sum |x| in [0,1] (if denom>0)
    denom = float(np.sum(a))
    D = float(np.abs(np.sum(x)) / denom) if denom > 0 else np.nan

    # --- RI components ---
    # First differences
    dx = np.diff(x)

    # (1) Reversal in differences: Gamma = (-corr(dx_t, dx_{t-1}) + 1)/2
    # Need at least 3 diffs (=> T>=4) to get corr(dx[1:], dx[:-1]) with len>=3
    corr_dx = _safe_corr(dx[1:], dx[:-1]) if len(dx) >= 3 else np.nan
    gamma = (-corr_dx) if not np.isnan(corr_dx) else np.nan
    Gamma = (gamma + 1.0) / 2.0 if not np.isnan(gamma) else np.nan

    # (2) Flip rate: fraction of consecutive days with different sign
    F = float(np.mean(s[1:] != s[:-1])) if T >= 2 else np.nan

    # (3) Alternation intensity: OFI magnitude occurring on flip transitions
    if T >= 2:
        flip = (s[1:] != s[:-1])
        denom2 = float(np.sum(a[1:]))
        A = float(np.sum(a[1:][flip]) / denom2) if denom2 > 0 else np.nan
    else:
        A = np.nan


    # --- PI Bayesian shrinkage weights ---

    # Reliability (effective sample size)
      w_rho = max(T - 3, 1)
      w_c   = max(T - 1, 1)
      w_d   = max(T, 1)

      pi_vals = np.array([rho1_01, C, D], dtype=float)

      # Intuitive priors (importance)
      pi_prior = np.array([0.45, 0.35, 0.20])

      # Bayesian posterior weights
      pi_wts_raw = pi_prior * np.array([w_rho, w_c, w_d], dtype=float)

      valid_pi = ~np.isnan(pi_vals)
      PI = (
          np.sum(pi_vals[valid_pi] * pi_wts_raw[valid_pi]) /
          np.sum(pi_wts_raw[valid_pi])
          if valid_pi.any()
          else np.nan
      )

    # --- RI Bayesian shrinkage weights ---

      Td = T - 1

      w_gamma = max(Td - 3, 1)
      w_f     = max(Td, 1)
      w_a     = max(Td, 1)

      ri_vals = np.array([Gamma, F, A], dtype=float)

      # Intuitive priors (importance)
      ri_prior = np.array([0.50, 0.30, 0.20])

      # Bayesian posterior weights
      ri_wts_raw = ri_prior * np.array([w_gamma, w_f, w_a], dtype=float)

      valid_ri = ~np.isnan(ri_vals)
      RI = (
          np.sum(ri_vals[valid_ri] * ri_wts_raw[valid_ri]) /
          np.sum(ri_wts_raw[valid_ri])
          if valid_ri.any()
          else np.nan
      )



    return {
        "PI": PI,
        "RI": RI,
        # keep components for sanity-check diagnostics
        "pi_rho1_01": rho1_01,
        "pi_consistency": C,
        "pi_drift_ratio": D,
        "ri_gamma_01": Gamma,
        "ri_flip_rate": F,
        "ri_alt_intensity": A,
        "T": T,
    }


def compute_pi_ri(
    daily_df: pd.DataFrame,
    event_col: str = "event_id",
    date_col: str = "date",
    ofi_col: str = "ofi",
    min_days: int = 7,
) -> pd.DataFrame:
    """
    Compute Persistence Index (PI) and Reversibility Index (RI) per event.

    Parameters
    ----------
    daily_df : DataFrame
        Must contain columns [event_col, date_col, ofi_col].
        Multiple rows per event; will be sorted by date within event.
    min_days : int
        Events with fewer than this many observations are dropped.

    Returns
    -------
    DataFrame with one row per event_id:
        [event_id, PI, RI, component columns..., T]
    """
    df = daily_df[[event_col, date_col, ofi_col]].copy()
    df[date_col] = pd.to_datetime(df[date_col])
    df = df.sort_values([event_col, date_col])

    # Filter by min days
    counts = df.groupby(event_col)[ofi_col].size()
    keep = counts[counts >= min_days].index
    df = df[df[event_col].isin(keep)]

    rows = []
    for eid, g in df.groupby(event_col, sort=False):
        res = _event_pi_ri(g[ofi_col])
        res[event_col] = eid
        rows.append(res)

    out = pd.DataFrame(rows)
    # Put event_id first
    cols = [event_col] + [c for c in out.columns if c != event_col]
    return out[cols]



def plot_component_distributions(
    df,
    components,
    bins=40,
    title_prefix="",
    clip=(0.0, 1.0),
    show=True,
):
    """
    Plot histogram distributions for a list of component columns.

    Parameters
    ----------
    df : pd.DataFrame
        Event-level dataframe containing PI/RI components.
    components : list[str]
        Column names to plot.
    bins : int
        Number of histogram bins.
    clip : tuple or None
        (min, max) to clip values before plotting; None disables clipping.
    """
    n = len(components)
    ncols = 3
    nrows = int(np.ceil(n / ncols))

    fig, axes = plt.subplots(nrows, ncols, figsize=(4.8 * ncols, 3.6 * nrows))
    axes = np.atleast_1d(axes).flatten()

    for ax, col in zip(axes, components):
        if col not in df.columns:
            ax.set_visible(False)
            continue

        x = df[col].dropna().values
        if clip is not None:
            x = np.clip(x, clip[0], clip[1])

        ax.hist(x, bins=bins, density=True, alpha=0.75)
        ax.set_title(col)
        ax.set_xlabel("Value")
        ax.set_ylabel("Density")

        # Reference lines for bounded indices
        if clip is not None:
            ax.axvline(clip[0], linestyle="--", linewidth=1)
            ax.axvline(clip[1], linestyle="--", linewidth=1)

        # Summary stats
        if len(x) > 0:
            mean = np.mean(x)
            ax.axvline(mean, linestyle="-", linewidth=1)
            ax.text(
                0.98,
                0.95,
                f"mean={mean:.2f}\nN={len(x)}",
                transform=ax.transAxes,
                ha="right",
                va="top",
                fontsize=9,
            )

    # Hide unused axes
    for ax in axes[len(components):]:
        ax.set_visible(False)

    fig.suptitle(f"{title_prefix} Component Distributions", fontsize=14)
    fig.tight_layout(rect=[0, 0, 1, 0.96])

    if show:
        plt.show()

    return fig

